{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1f5fc3-1902-4406-ad56-5d7882eb0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a09ad56-d5bf-48a6-89a2-31f0a7b16bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a123eef-0770-4a6c-8830-01af571eb78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.78862847],\n",
      "       [ 0.43650985],\n",
      "       [ 0.09649747],\n",
      "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
      "       [-0.35475898],\n",
      "       [-0.08274148],\n",
      "       [-0.62700068]]), array([[-0.04381817],\n",
      "       [-0.47721803],\n",
      "       [-1.31386475],\n",
      "       [ 0.88462238]])]\n"
     ]
    }
   ],
   "source": [
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "all_x = []\n",
    "for i in range(N):\n",
    "    all_x.append(np.random.normal(size=(D,1)))\n",
    "\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121f6bd6-cd37-48a3-b9c7-856d43f8f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "omega_q = np.random.normal(size=(D, D))\n",
    "omega_k = np.random.normal(size=(D, D))\n",
    "omega_v = np.random.normal(size=(D, D))\n",
    "beta_q = np.random.normal(size=(D,1))\n",
    "beta_k = np.random.normal(size=(D,1))\n",
    "beta_v = np.random.normal(size=(D,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6de0f7-5891-40e5-a406-6d48c19f13f9",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4549c3e-5e6d-4c08-947b-786b87b1212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "\n",
    "# 12.2 & 12.4\n",
    "\n",
    "for x in all_x:\n",
    "    query = beta_q + omega_q @ x\n",
    "    key = beta_k + omega_k @ x\n",
    "    value = beta_v + omega_v @ x\n",
    "\n",
    "    all_queries.append(query)\n",
    "    all_values.append(value)\n",
    "    all_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f546ab6-7452-4fb6-81f5-77c06fab38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(items_in):\n",
    "    items_out = np.exp(items_in) / np.sum(np.exp(items_in))\n",
    "    return items_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200fc62d-a7d0-47b1-b077-d1b9d9fe9ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-30.69506335]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(all_keys[0].T, all_queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ceab59-1638-4f66-85d6-e79e8c98d2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.69380505],\n",
       "        [-3.9952365 ],\n",
       "        [ 3.7499519 ],\n",
       "        [ 3.12154293]]),\n",
       " array([[-2.36543342],\n",
       "        [ 3.07476988],\n",
       "        [-3.59698468],\n",
       "        [ 1.22226059]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys[0], all_queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ffb41f-b0e0-4ce0-a5cd-9e6d14ac6f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentions for output  0\n",
      "[[[1.24326146e-13]]\n",
      "\n",
      " [[9.98281489e-01]]\n",
      "\n",
      " [[1.71851130e-03]]]\n",
      "Attentions for output  1\n",
      "[[[2.79525306e-12]]\n",
      "\n",
      " [[5.85506360e-03]]\n",
      "\n",
      " [[9.94144936e-01]]]\n",
      "Attentions for output  2\n",
      "[[[0.00505708]]\n",
      "\n",
      " [[0.00654776]]\n",
      "\n",
      " [[0.98839516]]]\n",
      "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ],
   "source": [
    "all_x_prime = []\n",
    "# For each output\n",
    "for n in range(N):\n",
    "    # Create list for dot products of query N with all keys\n",
    "    all_km_qn = []\n",
    "    for key in all_keys:\n",
    "        dot_product = np.dot(key.T, all_queries[n])\n",
    "        all_km_qn.append(dot_product)\n",
    "    \n",
    "    attention = softmax(all_km_qn)\n",
    "    print(\"Attentions for output \", n)\n",
    "    # should be positive sum to one\n",
    "    print(attention)\n",
    "\n",
    "    # Compute a weighted sum of all of the values according to the attention 12.3\n",
    "    x_prime = np.sum(attention * all_values, axis=0)\n",
    "        \n",
    "    all_x_prime.append(x_prime)\n",
    "\n",
    "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f0c3d-7829-4cb4-b45b-663b74e4ff99",
   "metadata": {},
   "source": [
    "#### Now let's compute the same thing, but using matrix calculations. We'll store the inputs in the columns of a D x N matrix, using equations 12.6 and 12.7/8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac0f0ff-8fc4-48ad-a415-408d1746f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # not required as broadcasting would take care of it\n",
    "  # Replicate denominator to N rows\n",
    "  # denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5738d53-a6c2-4b2d-9c57-9c3c5c484669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "    # 1. Compute queries, keys, and values\n",
    "    # 2. Compute dot products\n",
    "    # 3. Apply softmax to calculate attentions\n",
    "    # 4. Weight values by attentions\n",
    "    one = np.ones((N, 1))\n",
    "    queries = beta_q @ one.T + omega_q @ X\n",
    "    keys = beta_k @ one.T + omega_k @ X\n",
    "    values = beta_v @ one.T + omega_v @ X\n",
    "\n",
    "    dot_products = np.dot(keys.T, queries)\n",
    "    attention = softmax_cols(dot_products)\n",
    "    print(attention)\n",
    "    X_prime = values @ attention\n",
    "    \n",
    "    return X_prime\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f83500-2268-45ba-9af5-f3753b7ed499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.24326146e-13 2.79525306e-12 5.05707907e-03]\n",
      " [9.98281489e-01 5.85506360e-03 6.54776072e-03]\n",
      " [1.71851130e-03 9.94144936e-01 9.88395160e-01]]\n",
      "[[ 0.94744244  1.64201168  1.61949281]\n",
      " [-0.24348429 -0.08470004 -0.06641533]\n",
      " [-0.91310441  4.02764044  3.96863308]\n",
      " [-0.44522983  2.18690791  2.15858316]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Copy data into matrix\n",
    "X = np.zeros((D, N))\n",
    "X[:,0] = np.squeeze(all_x[0])\n",
    "X[:,1] = np.squeeze(all_x[1])\n",
    "X[:,2] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cda38-33ba-4156-a9f6-f6e9dcb94b4a",
   "metadata": {},
   "source": [
    "\n",
    "#### Printing out the attention matrix we see that the values are quite extreme, one is very close to one and the others are very close to zero. \n",
    "\n",
    "#### Now we'll fix this problem by using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3dd6fd-3562-4ee7-be62-ba7c1826dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "\n",
    "    # 1. Compute queries, keys, and values\n",
    "    # 2. Compute dot products\n",
    "    # 3. Scale the dot products as in equation 12.9\n",
    "    # 4. Apply softmax to calculate attentions\n",
    "    # 5. Weight values by attentions\n",
    "    one = np.ones((N, 1))\n",
    "    queries = beta_q @ one.T + omega_q @ X\n",
    "    keys = beta_k @ one.T + omega_k @ X\n",
    "    values = beta_v @ one.T + omega_v @ X\n",
    "\n",
    "    dot_products = np.dot(keys.T, queries)\n",
    "    attention = softmax_cols(dot_products / np.sqrt(queries.shape[0]))\n",
    "    print(attention)\n",
    "    X_prime = values @ attention\n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb041d9e-377a-4d95-a064-56cce2ba9fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.38843552e-07 1.55730194e-06 6.20418746e-02]\n",
      " [9.60161968e-01 7.12734969e-02 7.05962187e-02]\n",
      " [3.98376935e-02 9.28724946e-01 8.67361907e-01]]\n",
      "[[ 0.97411966  1.59622051  1.32638014]\n",
      " [-0.23738409 -0.09516106  0.13062402]\n",
      " [-0.72333202  3.70194096  3.02371664]\n",
      " [-0.34413007  2.01339538  1.6902419 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b04410-cdfb-4d00-bb27-961629cc95d3",
   "metadata": {},
   "source": [
    "## Multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1a0ad8-f952-43de-bc5f-cadd60e3c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.78862847  0.43650985  0.09649747 -1.8634927  -0.2773882  -0.35475898]\n",
      " [-0.08274148 -0.62700068 -0.04381817 -0.47721803 -1.31386475  0.88462238]\n",
      " [ 0.88131804  1.70957306  0.05003364 -0.40467741 -0.54535995 -1.54647732]\n",
      " [ 0.98236743 -1.10106763 -1.18504653 -0.2056499   1.48614836  0.23671627]\n",
      " [-1.02378514 -0.7129932   0.62524497 -0.16051336 -0.76883635 -0.23003072]\n",
      " [ 0.74505627  1.97611078 -1.24412333 -0.62641691 -0.80376609 -2.41908317]\n",
      " [-0.92379202 -1.02387576  1.12397796 -0.13191423 -1.62328545  0.64667545]\n",
      " [-0.35627076 -1.74314104 -0.59664964 -0.58859438 -0.8738823   0.02971382]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 6\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "# Create an empty list\n",
    "X = np.random.normal(size=(D,N))\n",
    "# Print X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e57b40c-242c-46ef-a593-d6d15013bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of heads\n",
    "H = 2\n",
    "# QDV dimension\n",
    "H_D = int(D/H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "omega_q1 = np.random.normal(size=(H_D,D))\n",
    "omega_k1 = np.random.normal(size=(H_D,D))\n",
    "omega_v1 = np.random.normal(size=(H_D,D))\n",
    "beta_q1 = np.random.normal(size=(H_D,1))\n",
    "beta_k1 = np.random.normal(size=(H_D,1))\n",
    "beta_v1 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "omega_q2 = np.random.normal(size=(H_D,D))\n",
    "omega_k2 = np.random.normal(size=(H_D,D))\n",
    "omega_v2 = np.random.normal(size=(H_D,D))\n",
    "beta_q2 = np.random.normal(size=(H_D,1))\n",
    "beta_k2 = np.random.normal(size=(H_D,1))\n",
    "beta_v2 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_c = np.random.normal(size=(D,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "759db34c-fd77-4e21-b2fb-796fb9aac80d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
    "\n",
    "    sa1 = scaled_dot_product_self_attention(X, omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1)\n",
    "    sa2 = scaled_dot_product_self_attention(X, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2)\n",
    "\n",
    "    X_prime = omega_c @ np.vstack((sa1, sa2))\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8859b772-4f94-4d88-a49f-e1a418209b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.451e-15 4.457e-14 2.405e-03 9.720e-01 8.643e-02 5.027e-01]\n",
      " [1.125e-14 6.564e-17 3.265e-03 2.318e-06 3.599e-04 7.835e-04]\n",
      " [1.658e-12 1.384e-08 1.481e-03 2.902e-03 1.048e-03 1.122e-03]\n",
      " [5.500e-03 8.826e-08 1.588e-01 2.611e-07 2.639e-02 4.114e-02]\n",
      " [9.945e-01 1.109e-09 8.290e-01 2.149e-07 8.553e-01 4.541e-01]\n",
      " [6.352e-06 1.000e+00 5.036e-03 2.509e-02 3.047e-02 1.798e-04]]\n",
      "[[2.543e-03 3.404e-02 1.329e-03 9.340e-03 3.044e-02 7.030e-03]\n",
      " [3.154e-03 9.386e-01 1.270e-04 2.490e-01 9.600e-01 2.895e-03]\n",
      " [7.096e-07 2.436e-02 9.532e-02 7.415e-01 9.580e-03 3.025e-01]\n",
      " [6.826e-01 2.964e-03 6.408e-05 2.040e-09 2.518e-07 1.759e-05]\n",
      " [3.117e-01 6.532e-06 2.764e-04 3.434e-10 1.232e-07 1.207e-04]\n",
      " [1.807e-07 5.470e-06 9.029e-01 2.150e-04 5.442e-06 6.874e-01]]\n",
      "Your answer:\n",
      "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
      " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
      " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
      " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
      " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
      " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
      " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
      " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n",
      "True values:\n",
      "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
      " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
      " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
      " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
      " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
      " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
      " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
      " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n"
     ]
    }
   ],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
    "\n",
    "# Print out the results\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"True values:\")\n",
    "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
    "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
    "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
    "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
    "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
    "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
    "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
    "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce6bd6-0124-48e9-9a51-946108e3166e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
